{"./":{"url":"./","title":"序言","keywords":"","body":"Istio Handbook——Istio中文指南/服务网格实践手册 Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017年初开始进入大众视野。本书是 Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册的续篇，Kubernetes 解决了云原生应用的部署问题，Istio 解决是应用的服务（流量）治理问题。在 Kubernetes Handbook 中有大量的关于 Istio 和服务网格相关的章节，这些内容已经与 Kubernetes 本身没有太强的关系，随着 2018年7月31日 Istio 1.0 发布，Istio 本身已经日趋稳定，作为 ServiceMesher 社区 的联合创始人我也在社区活动中积累的大量的资料，为了回馈社区，我决定组合社区中已有的资料加上个人撰写，将服务网格部分独立出来单独成书。 本书的主题包括： 服务网格概念解析 Istio 架构详解 Istio 实战 企业采用服务网格的路径 本书基于 Istio 1.0+ 版本编写，您可以通过以下地址参与到本书的编写或阅读本书： GitHub 地址：https://github.com/rootsongjc/istio-handbook Gitbook 在线浏览：https://jimmysong.io/istio-handbook/ 快速开始 阅读本书前希望您有容器和 Kubernetes 的基础知识，如果您想要从零开始，那么可以使用 kubernetes-vagrant-centos-cluster 并运行 Bookinfo 应用来快速体验服务网格。 致谢 ServiceMesher 社区 负责翻译了 Envoy 官方文档，并负责了 Istio 官方中文文档的维护，同时还编译了大量资料，本书所有文章的文末参考栏目里标注了参考文章的链接，感谢大家对本书的大力支持。 参与本书 请参考 Istio 网站样式指南。 ServiceMesher 社区 更多关于服务网格的资讯、技术干货请关注我们的社区，联系我加入社区。 社区网站：http://www.servicemesher.com 微信公众号 Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:33:32 "},"concepts/what-is-service-mesh.html":{"url":"concepts/what-is-service-mesh.html","title":"什么是服务网格？","keywords":"","body":"什么是服务网格？ Service mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO 服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。 服务网格的特点 服务网格有如下几个特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的服务网格开源软件 Istio 和 Istio 都可以直接在 kubernetes 中集成，其中 Istio 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 1.0。 理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。 Phil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉： 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现 服务网格的架构如下图所示： 图片 - Service Mesh 架构图 图片来自：Pattern: Service Mesh 服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。 服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。 Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。 在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。 在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。 参考 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? - buoyant.io So what even is a Service Mesh? Hot take on Istio and Istio - redmonk.com Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - servicemesher.com Istio 官方文档 - istio.io Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:22:26 "},"concepts/service-mesh-architectures.html":{"url":"concepts/service-mesh-architectures.html","title":"服务网格架构","keywords":"","body":"服务网格架构 下图是Conduit Service Mesh（现在已合并到Linkerd2中了）的架构图，这是Service Mesh的一种典型的架构。 服务网格中分为控制平面和数据平面，当前流行的两款开源的服务网格 Istio 和 Linkerd 实际上都是这种构造，只不过 Istio 的划分更清晰，而且部署更零散，很多组件都被拆分，控制平面中包括 Mixer、Pilot、Citadel，数据平面默认是用Envoy；而 Linkerd 中只分为 Linkerd 做数据平面，namerd 作为控制平面。 控制平面 控制平面的特点： 不直接解析数据包 与控制平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供API或者命令行工具可用于配置版本化管理，便于持续集成和部署 数据平面 数据平面的特点： 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署 参考 企业级服务网格架构之路解读 Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:03:28 "},"concepts/service-mesh-patterns.html":{"url":"concepts/service-mesh-patterns.html","title":"服务网格的实现模式","keywords":"","body":"服务网格的实现模式 我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成Service Mesh架构前使用微服务架构通常的形式，下图是使用Service Mesh架构的最终形式。 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。 Ingress或边缘代理 如果你使用的是Kubernetes做容器编排调度，那么在进化到Service Mesh架构之前，通常会使用Ingress Controller，做集群内外流量的反向代理，如使用Traefik或Nginx Ingress Controller。 这样只要利用Kubernetes的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要L7代理的话这种改造十分简单，但问题是无法管理服务间流量。 路由器网格 Ingress或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个Router层，即路由器层，让集群内所有服务间的流量都通过该路由器。 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。 Proxy per Node 这种架构是在每个节点上都部署一个代理，如果使用Kubernetes来部署的话就是使用DaemonSet对象，Linkerd第一代就是使用这种方式部署的，一代的Linkerd使用Scala开发，基于JVM比较消耗资源，二代的Linkerd使用Go开发。 这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个sidecar的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。 Sidecar代理/Fabric模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。 这已经是最接近Service Mesh架构的一种形态了，唯一缺的就是控制平面了。所有的sidecar都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多sidecar就需要一个统一的控制平面了。 Sidecar代理/控制平面 下面的示意图是目前大多数Service Mesh的架构图，也可以说是整个Service Mesh架构演进的最终形态。 这种架构将代理作为整个服务网格中的一部分，使用Kubernetes部署的话，可以通过以sidecar的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。 多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。 参考 企业级服务网格架构之路解读 Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:03:36 "},"concepts/istio-architecture.html":{"url":"concepts/istio-architecture.html","title":"Istio 架构解析","keywords":"","body":"Istio 架构解析 参考 Isito 是什么? - istio.io Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:39:05 "},"concepts/sidecar-pattern.html":{"url":"concepts/sidecar-pattern.html","title":"Sidecar 模式","keywords":"","body":"Sidecar 模式 Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。 就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。 让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。 每个微服务都需要具有可观察性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。 但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。 使用 Sidecar 模式的优势 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。 Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的 Service Mesh 架构。 使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。 例如下图 SOFAMesh & SOFA MOSN—基于Istio构建的用于应对大规模流量的Service Mesh解决方案的架构图中描述的，MOSN 作为 Sidecar 的方式和应用运行在同一个 Pod 中，拦截所有进出应用容器的流量，SOFAMesh 兼容 Istio，其中使用 Go 语言开发的 SOFAMosn 替换了 Envoy。 图片 - SOFAMesh 架构图 参考 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - servicemesher.com Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:24:03 "},"concepts/sidecar-injection-deep-dive.html":{"url":"concepts/sidecar-injection-deep-dive.html","title":"Istio 中的 Sidecar 注入详解","keywords":"","body":"Sidecar 注入详解 在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念： Sidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 Init 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。 关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。 Sidecar 注入示例分析 我们看下 Istio 官方示例 bookinfo 中 productpage 的 YAML 配置，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: productpage-v1 spec: replicas: 1 template: metadata: labels: app: productpage version: v1 spec: containers: - name: productpage image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 再查看下 productpage 容器的 Dockerfile。 FROM python:2.7-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY productpage.py /opt/microservices/ COPY templates /opt/microservices/templates COPY requirements.txt /opt/microservices/ EXPOSE 9080 WORKDIR /opt/microservices CMD python productpage.py 9080 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。 $ istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml 我们只截取其中与 productpage 相关的 Service 和 Deployment 配置部分。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: creationTimestamp: null name: productpage-v1 spec: replicas: 1 strategy: {} template: metadata: annotations: sidecar.istio.io/status: '{\"version\":\"fde14299e2ae804b95be08e0f2d171d466f47983391c00519bbf01392d9ad6bb\",\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"],\"imagePullSecrets\":null}' creationTimestamp: null labels: app: productpage version: v1 spec: containers: - image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent name: productpage ports: - containerPort: 9080 resources: {} - args: - proxy - sidecar - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istio-pilot.istio-system:15007 - --discoveryRefreshDelay - 1s - --zipkinAddress - zipkin.istio-system:9411 - --connectTimeout - 10s - --statsdUdpAddress - istio-statsd-prom-bridge.istio-system:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: REDIRECT image: jimmysong/istio-release-proxyv2:1.0.0 imagePullPolicy: IfNotPresent name: istio-proxy resources: requests: cpu: 10m securityContext: privileged: false readOnlyRootFilesystem: true runAsUser: 1337 volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true initContainers: - args: - -p - \"15001\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - 9080, - -d - \"\" image: jimmysong/istio-release-proxy_init:1.0.0 imagePullPolicy: IfNotPresent name: istio-init resources: {} securityContext: capabilities: add: - NET_ADMIN privileged: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true secretName: istio.default status: {} 我们看到 Service 的配置没有变化，所有的变化都在 Deployment 里，Istio 给应用 Pod 注入的配置主要包括： Init 容器 istio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 Envoy sidecar 容器 istio-proxy：运行 Envoy 代理 接下来将分别解析下这两个容器。 Init 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动参数： -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是什么以确定启动时执行的命令。 FROM ubuntu:xenial RUN apt-get update && apt-get install -y \\ iproute2 \\ iptables \\ && rm -rf /var/lib/apt/lists/* ADD istio-iptables.sh /usr/local/bin/ ENTRYPOINT [\"/usr/local/bin/istio-iptables.sh\"] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables.sh 脚本，再按图索骥看看这个脚本里到底写的什么，该脚本的位置在 Istio 源码仓库的 tools/deb/istio-iptables.sh，一共 300 多行，就不贴在这里了。下面我们就来解析下这个启动脚本。 Init 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptables.sh 脚本，该脚本的用法如下： $ istio-iptables.sh -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 环境变量位于 $ISTIO_SIDECAR_CONFIG（默认在：/var/lib/istio/envoy/sidecar.env） 通过查看该脚本你将看到，以上传入的参数都会重新组装成 iptables 命令的参数。 再参考 istio-init 容器的启动参数，完整的启动命令如下： $ /usr/local/bin/istio-iptables.sh -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。 命令解析 这条启动命令的作用是： 将应用容器的所有流量都转发到 Envoy 的 15001 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将所有访问 9080 端口（即应用容器 productpage 的端口）的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。 istio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage Pod 所在的主机上使用 docker 命令登陆容器中查看。 查看 productpage Pod 所在的主机。 $ kubectl -n default get pod -l app=productpage -o wide NAME READY STATUS RESTARTS AGE IP NODE productpage-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。 $ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为 “REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables，规则配置请参考 iptables 规则配置。 理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。 下图展示了 iptables 调用链。 图片 - iptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表： raw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换（例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。 不同的表中的具有的链类型如下表所示： 规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ ✓ 下图是 iptables 的调用链顺序。 图片 - iptables 调用链 关于 iptables 的详细介绍请参考常见 iptables 使用规则场景整理。 iptables 命令 iptables 命令的主要用途是修改这些表中的规则。iptables 命令格式如下： $ iptables [-t 表名] 命令选项［链名]［条件匹配］[-j 目标动作或跳转］ Init 容器中的 /istio-iptables.sh 启动入口脚本就是执行 iptables 初始化的。 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。 $ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。 下图是 iptables 的建议结构图，流量在经过 INPUT 链之后就进入了上层协议栈，比如 图片 - iptables结构图 图片来自常见 iptables 使用规则场景整理 每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。 pkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，后者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。 关于 iptables 规则请参考常见iptables使用规则场景整理。 target 支持的类型 target 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念。 从输出结果中可以看到 Init 容器没有在 iptables 的默认链路中创建任何规则，而是创建了新的链路。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。 图片 - Envoy sidecar 流量劫持 Istio iptables 宋净超 Jimmy Song 服务网格 Service Mesh Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。下面是查看 nat 表中的规则，其中链的名字中包含 ISTIO 前缀的是由 Init 容器注入的，规则匹配是根据下面显示的顺序来执行的，其中会有多次跳转。 # 查看 NAT 表中规则配置的详细信息 $ iptables -t nat -L -v # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上 Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 2 120 ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链 Chain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上 Chain OUTPUT (policy ACCEPT 41146 packets, 3845K bytes) pkts bytes target prot opt in out source destination 93 5580 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理 Chain POSTROUTING (policy ACCEPT 41199 packets, 3848K bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有目的地为 9080 端口的入站流量重定向到 ISTIO_IN_REDIRECT 链上 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 2 120 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere tcp dpt:9080 # ISTIO_IN_REDIRECT 链：将所有的入站流量跳转到本地的 15001 端口，至此成功的拦截了流量到 Envoy Chain ISTIO_IN_REDIRECT (1 references) pkts bytes target prot opt in out source destination 2 120 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUPT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 ISTIO_REDIRECT all -- any lo anywhere !localhost 40 2400 RETURN all -- any any anywhere anywhere owner UID match istio-proxy 0 0 RETURN all -- any any anywhere anywhere owner GID match istio-proxy 0 0 RETURN all -- any any anywhere localhost 53 3180 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT 链：将所有流量重定向到 Envoy（即本地） 的 15001 端口 Chain ISTIO_REDIRECT (2 references) pkts bytes target prot opt in out source destination 53 3180 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 iptables 显示的链的顺序，即流量规则匹配的顺序。其中要特别注意 ISTIO_OUTPUT 链中的规则配置。为了避免流量一直在 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。 ISTIO_OUTPUT 链规则匹配的详细过程如下： 如果目的地非 localhost 就跳转到 ISTIO_REDIRECT 链 所有来自 istio-proxy 用户空间的非 localhost 流量跳转到它的调用点 OUTPUT 继续执行 OUTPUT 链的下一条规则，因为 OUTPUT 链中没有下一条规则了，所以会继续执行 POSTROUTING 链然后跳出 iptables，直接访问目的地 如果流量不是来自 istio-proxy 用户空间，又是对 localhost 的访问，那么就跳出 iptables，直接访问目的地 其它所有情况都跳转到 ISTIO_REDIRECT 链 其实在最后这条规则前还可以增加 IP 地址过滤，让某些 IP 地址段不通过 Envoy 代理。 图片 - istio sidecar iptables 注入 以上 iptables 规则都是 Init 容器启动的时使用 istio-iptables.sh 脚本生成的，详细过程可以查看该脚本。 查看 Envoy 运行状态 首先查看 proxyv2 镜像的 Dockerfile。 FROM istionightly/base_debug ARG proxy_version ARG istio_version # 安装 Envoy ADD envoy /usr/local/bin/envoy # 使用环境变量的方式明文指定 proxy 的版本/功能 ENV ISTIO_META_ISTIO_PROXY_VERSION \"1.1.0\" # 使用环境变量的方式明文指定 proxy 明确的 sha，用于指定版本的配置和调试 ENV ISTIO_META_ISTIO_PROXY_SHA $proxy_version # 环境变量，指定明确的构建号，用于调试 ENV ISTIO_META_ISTIO_VERSION $istio_version ADD pilot-agent /usr/local/bin/pilot-agent ADD envoy_pilot.yaml.tmpl /etc/istio/proxy/envoy_pilot.yaml.tmpl ADD envoy_policy.yaml.tmpl /etc/istio/proxy/envoy_policy.yaml.tmpl ADD envoy_telemetry.yaml.tmpl /etc/istio/proxy/envoy_telemetry.yaml.tmpl ADD istio-iptables.sh /usr/local/bin/istio-iptables.sh COPY envoy_bootstrap_v2.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json RUN chmod 755 /usr/local/bin/envoy /usr/local/bin/pilot-agent # 将 istio-proxy 用户加入 sudo 权限以允许执行 tcpdump 和其他调试命令 RUN useradd -m --uid 1337 istio-proxy && \\ echo \"istio-proxy ALL=NOPASSWD: ALL\" >> /etc/sudoers && \\ chown -R istio-proxy /var/lib/istio # 使用 pilot-agent 来启动 Envoy ENTRYPOINT [\"/usr/local/bin/pilot-agent\"] 该容器的启动入口是 pilot-agent 命令，根据 YAML 配置中传递的参数，详细的启动命令入下： /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --statsdUdpAddress istio-statsd-prom-bridge.istio-system:9125 --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE 主要配置了 Envoy 二进制文件的位置、服务发现地址、服务集群名、监控指标上报地址、Envoy 的管理端口、热重启时间等，详细用法请参考 Istio官方文档 pilot-agent 的用法。 pilot-agent 是容器中 PID 为 1 的启动进程，它启动时又创建了一个 Envoy 进程，如下： /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local --max-obj-name-len 189 -l warn --v2-config-only 我们分别解释下以上配置的意义。 -c /etc/istio/proxy/envoy-rev0.json：配置文件，支持 .json、.yaml、.pb 和 .pb_text 格式，pilot-agent 启动的时候读取了容器的环境变量后创建的。 --restart-epoch 0：Envoy 热重启周期，第一次启动默认为 0，每热重启一次该值加 1。 --drain-time-s 45：热重启期间 Envoy 将耗尽连接的时间。 --parent-shutdown-time-s 60： Envoy 在热重启时关闭父进程之前等待的时间。 --service-cluster productpage：Envoy 运行的本地服务集群的名字。 --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local：定义 Envoy 运行的本地服务节点名称，其中包含了该 Pod 的名称、IP、DNS 域等信息，根据容器的环境变量拼出来的。 -max-obj-name-len 189：cluster/route_config/listener 中名称字段的最大长度（以字节为单位） -l warn：日志级别 --v2-config-only：只解析 v2 引导配置文件 详细配置请参考 Envoy 的命令行选项。 查看 Envoy 的配置文件 /etc/istio/proxy/envoy-rev0.json。 { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" } }, \"stats_config\": { \"use_all_default_tags\": false }, \"admin\": { \"access_log_path\": \"/dev/stdout\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } }, \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"refresh_delay\": {\"seconds\": 1, \"nanos\": 0}, \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } }, \"static_resources\": { \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connect_timeout\": {\"seconds\": 10, \"nanos\": 0}, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"istio-pilot.istio-system\", \"port_value\": 15010} } ], \"circuit_breakers\": { \"thresholds\": [ { \"priority\": \"default\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }, { \"priority\": \"high\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }] }, \"upstream_connection_options\": { \"tcp_keepalive\": { \"keepalive_time\": 300 } }, \"http2_protocol_options\": { } } , { \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connect_timeout\": { \"seconds\": 1 }, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"zipkin.istio-system\", \"port_value\": 9411} } ] } ] }, \"tracing\": { \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"stats_sinks\": [ { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": {\"address\": \"10.254.109.175\", \"port_value\": 9125} } } } ] } 下图是使用 Istio 管理的 bookinfo 示例的访问请求路径图。 图片 - Istio bookinfo 图片来自 Istio 官方网站 对照 bookinfo 示例的 productpage 的查看建立的连接。在 productpage-v1-745ffc55b7-2l2lw Pod 的 istio-proxy 容器中使用 root 用户查看打开的端口。 $ lsof -i COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME envoy 11 istio-proxy 9u IPv4 73951 0t0 TCP localhost:15000 (LISTEN) # Envoy admin 端口 envoy 11 istio-proxy 17u IPv4 74320 0t0 TCP productpage-v1-745ffc55b7-2l2lw:46862->istio-pilot.istio-system.svc.cluster.local:15010 (ESTABLISHED) # 15010：istio-pilot 的 grcp-xds 端口 envoy 11 istio-proxy 18u IPv4 73986 0t0 UDP productpage-v1-745ffc55b7-2l2lw:44332->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 的端口 envoy 11 istio-proxy 52u IPv4 74599 0t0 TCP *:15001 (LISTEN) # Envoy 的监听端口 envoy 11 istio-proxy 53u IPv4 74600 0t0 UDP productpage-v1-745ffc55b7-2l2lw:48011->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 端口 envoy 11 istio-proxy 54u IPv4 338551 0t0 TCP productpage-v1-745ffc55b7-2l2lw:15001->172.17.8.102:52670 (ESTABLISHED) # 52670：Ingress gateway 端口 envoy 11 istio-proxy 55u IPv4 338364 0t0 TCP productpage-v1-745ffc55b7-2l2lw:44046->172.33.78.9:9091 (ESTABLISHED) # 9091：istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 56u IPv4 338473 0t0 TCP productpage-v1-745ffc55b7-2l2lw:47210->zipkin.istio-system.svc.cluster.local:9411 (ESTABLISHED) # 9411: zipkin 端口 envoy 11 istio-proxy 58u IPv4 338383 0t0 TCP productpage-v1-745ffc55b7-2l2lw:41564->172.33.84.8:9080 (ESTABLISHED) # 9080：details-v1 的 http 端口 envoy 11 istio-proxy 59u IPv4 338390 0t0 TCP productpage-v1-745ffc55b7-2l2lw:54410->172.33.78.5:9080 (ESTABLISHED) # 9080：reivews-v2 的 http 端口 envoy 11 istio-proxy 60u IPv4 338411 0t0 TCP productpage-v1-745ffc55b7-2l2lw:35200->172.33.84.5:9091 (ESTABLISHED) # 9091:istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 62u IPv4 338497 0t0 TCP productpage-v1-745ffc55b7-2l2lw:34402->172.33.84.9:9080 (ESTABLISHED) # reviews-v1 的 http 端口 envoy 11 istio-proxy 63u IPv4 338525 0t0 TCP productpage-v1-745ffc55b7-2l2lw:50592->172.33.71.5:9080 (ESTABLISHED) # reviews-v3 的 http 端口 从输出级过上可以验证 Sidecar 是如何接管流量和与 istio-pilot 通信，及向 Mixer 做遥测数据汇聚的。感兴趣的读者可以再去看看其他几个服务的 istio-proxy 容器中的 iptables 和端口信息。 参考 SOFAMesh & SOFA MOSN—基于Istio构建的用于应对大规模流量的Service Mesh解决方案 - jimmysong.io Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 - jimmysong.io JSONPath Support - kubernetes.io iptables 命令使用说明 - wangchujiang.com How To List and Delete Iptables Firewall Rules - digitalocean.com 一句一句解说 iptables的详细中文手册 - cnblog.com 常见iptables使用规则场景整理 - aliang.org 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-21 00:11:49 "},"concepts/envoy-xds.html":{"url":"concepts/envoy-xds.html","title":"Envoy xDS","keywords":"","body":"Envoy xDS Envoy xDS 为 Istio 控制平面与控制平面通信的基本协议，只要代理支持该协议表达形式就可以创建自己 Sidecar 来替换 Envoy。这一章中将带大家了解 Envoy xDS。 Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:36:51 "},"setup/quick-start.html":{"url":"setup/quick-start.html","title":"快速开始","keywords":"","body":"快速开始 在使用 Istio 前还是希望您有容器和 Kubernetes 的基础知识，如果您想要从零开始，那么可以使用 kubernetes-vagrant-centos-cluster 并运行 Bookinfo 应用来快速体验服务网格。 准备环境 需要准备以下软件和环境： 8G以上内存 Vagrant 2.0+ VirtualBox 5.0 + 提前下载Kubernetes 1.9以上版本（支持最新的1.11.0）的release压缩包 Mac/Linux，不支持Windows 集群 我们使用 Vagrant 和 Virtualbox 安装包含 3 个节点的 Kubernetes 集群，其中 master 节点同时作为 node 节点。 IP 主机名 组件 172.17.8.101 node1 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、kubelet、docker、flannel、dashboard 172.17.8.102 node2 kubelet、docker、flannel、traefik 172.17.8.103 node3 kubelet、docker、flannel 注意：以上的IP、主机名和组件都是固定在这些节点的，即使销毁后下次使用vagrant重建依然保持不变。 容器 IP 地址范围：172.33.0.0/30 Kubernetes service IP 地址范围：10.254.0.0/16 安装的组件 安装完成后的集群包含以下组件： flannel（host-gw模式） etcd（单节点） kubectl CoreDNS Kubernetes（版本根据下载的 kubernetes 安装包而定，支持 Kubernetes 1.9+） Istio 可选插件 Kubernetes dashboard Helm Vistio Kiali 使用说明 将该repo克隆到本地，下载Kubernetes的到项目的根目录。 git clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git cd kubernetes-vagrant-centos-cluster wget https://storage.googleapis.com/kubernetes-release/release/v1.11.0/kubernetes-server-linux-amd64.tar.gz 注：您可以在这里找到Kubernetes的发行版下载地址。 使用vagrant启动集群。 vagrant up 如果是首次部署，会自动下载centos/7的box，这需要花费一些时间，另外每个节点还需要下载安装一系列软件包，整个过程大概需要10几分钟。 如果您在运行vagrant up的过程中发现无法下载centos/7的box，可以手动下载后将其添加到vagrant中。 手动添加centos/7 box wget -c http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box vagrant box add CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box --name centos/7 这样下次运行vagrant up的时候就会自动读取本地的centos/7 box而不会再到网上下载。 访问kubernetes集群 访问Kubernetes集群的方式有三种： 本地访问 在VM内部访问 Kubernetes dashboard 通过本地访问 可以直接在你自己的本地环境中操作该kubernetes集群，而无需登录到虚拟机中。 要想在本地直接操作Kubernetes集群，需要在你的电脑里安装kubectl命令行工具，对于Mac用户执行以下步骤： wget https://storage.googleapis.com/kubernetes-release/release/v1.11.0/kubernetes-client-darwin-amd64.tar.gz tar xvf kubernetes-client-darwin-amd64.tar.gz && cp kubernetes/client/bin/kubectl /usr/local/bin 将conf/admin.kubeconfig文件放到~/.kube/config目录下即可在本地使用kubectl命令操作集群。 mkdir -p ~/.kube cp conf/admin.kubeconfig ~/.kube/config 我们推荐您使用这种方式。 在虚拟机内部访问 如果有任何问题可以登录到虚拟机内部调试： vagrant ssh node1 sudo -i kubectl get nodes Kubernetes dashboard（可选安装） 还可以直接通过dashboard UI来访问：https://172.17.8.101:8443 可以在本地执行以下命令获取token的值（需要提前安装kubectl）： kubectl -n kube-system describe secret `kubectl -n kube-system get secret|grep admin-token|cut -d \" \" -f1`|grep \"token:\"|tr -s \" \"|cut -d \" \" -f2 注意：token的值也可以在vagrant up的日志的最后看到。 图片 - Kubernetes dashboard 只有当你安装了下面的heapster组件后才能看到上图中的监控metrics。 组件 Traefik 部署Traefik ingress controller和增加ingress配置： kubectl apply -f addon/traefik-ingress 在本地/etc/hosts中增加一条配置： 172.17.8.102 traefik.jimmysong.io 访问Traefik UI：http://traefik.jimmysong.io 图片 - Traefik Ingress controller Helm 用来部署helm。 hack/deploy-helm.sh 安装 Istio 我们以安装 istio 1.0 为例。 安装 到 Istio release 页面下载 istio 的安装包，安装 istio 命令行工具，将 istioctl 命令行工具放到你的$PATH目录下，对于 Mac 用户： wget https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-osx.tar.gz tar xvf istio-1.0.0-osx.tar.gz mv bin/istioctl /usr/local/bin/ 在 Kubernetes 中部署 istio： kubectl apply -f addon/istio/ 运行示例 kubectl apply -n default -f 在您自己的本地主机的/etc/hosts文件中增加如下配置项。 172.17.8.102 grafana.istio.jimmysong.io 172.17.8.102 servicegraph.istio.jimmysong.io 我们可以通过下面的URL地址访问以上的服务。 Service URL grafana http://grafana.istio.jimmysong.io servicegraph http://servicegraph.istio.jimmysong.io/dotviz, http://servicegraph.istio.jimmysong.io/graph,http://servicegraph.istio.jimmysong.io/force/forcegraph.html tracing http://172.17.8.101:31888 productpage http://172.17.8.101:31380/productpage 详细信息请参阅 https://istio.io/docs/guides/bookinfo.html 图片 - Bookinfo Demo Vistio（可选安装） Vizceral是Netflix发布的一个开源项目，用于近乎实时地监控应用程序和集群之间的网络流量。Vistio是使用Vizceral对Istio和网格监控的改进。它利用Istio Mixer生成的指标，然后将其输入Prometheus。Vistio查询Prometheus并将数据存储在本地以允许重播流量。 # Deploy vistio via kubectl kubectl -n default apply -f addon/vistio/ # Expose vistio-api kubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-api -o jsonpath='{.items[0].metadata.name}') 9091:9091 & # Expose vistio in another terminal window kubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-web -o jsonpath='{.items[0].metadata.name}') 8080:8080 & 如果一切都已经启动并准备就绪，您就可以访问Vistio UI，开始探索服务网格网络，访问http://localhost:8080 您将会看到类似下图的输出。 图片 - vistio 页面 更多详细内容请参考Vistio—使用Netflix的Vizceral可视化Istio service mesh。 Kiali（可选安装） Kiali是一个用于提供Istio service mesh观察性的项目，更多信息请查看https://kiali.io。 在本地该项目的根路径下执行下面的命令： kubectl apply -n istio-system -f addon/kiali 图片 - kiali 注意：当前还不支持jeager追踪，请使用上文中提到的jeager地址。 Weave scope（可选安装） Weave scope可用于监控、可视化和管理Docker&Kubernetes集群，详情见https://www.weave.works/oss/scope/ 在本地该项目的根路径下执行下面的命令： kubectl apply -f addon/weave-scope 在本地的/etc/hosts下增加一条记录。 172.17.8.102 scope.weave.jimmysong.io 现在打开浏览器，访问http://scope.weave.jimmysong.io/ 图片 - Weave scope动画 管理 除了特别说明，以下命令都在当前的repo目录下操作。 挂起 将当前的虚拟机挂起，以便下次恢复。 vagrant suspend 恢复 恢复虚拟机的上次状态。 vagrant resume 注意：我们每次挂起虚拟机后再重新启动它们的时候，看到的虚拟机中的时间依然是挂载时候的时间，这样将导致监控查看起来比较麻烦。因此请考虑先停机再重新启动虚拟机。 重启 停机后重启启动。 vagrant halt vagrant up # login to node1 vagrant ssh node1 # run the prosivision scripts /vagrant/hack/k8s-init.sh exit # login to node2 vagrant ssh node2 # run the prosivision scripts /vagrant/hack/k8s-init.sh exit # login to node3 vagrant ssh node3 # run the prosivision scripts /vagrant/hack/k8s-init.sh sudo -i cd /vagrant/hack ./deploy-base-services.sh exit 现在你已经拥有一个完整的基础的kubernetes运行环境，在该repo的根目录下执行下面的命令可以获取kubernetes dahsboard的admin用户的token。 hack/get-dashboard-token.sh 根据提示登录即可。 清理 清理虚拟机。 vagrant destroy rm -rf .vagrant 参考 Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册 rootsongjc/kubernetes-vagrant-centos-cluster - github.com duffqiu/centos-vagrant - github.com coredns/deployment - github.com Kubernetes 1.8 kube-proxy 开启 ipvs - mritd.me Vistio—使用Netflix的Vizceral可视化Istio service mesh - servicemesher.com Copyright © jimmysong.io 2018 all right reserved，powered by Gitbook Updated at 2018-09-20 20:51:33 "}}